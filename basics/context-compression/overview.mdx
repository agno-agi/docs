---
title: Context Compression
sidebarTitle: Overview
description: Learn how to compress tool call results to save context space while preserving critical information.
keywords: [context compression, tool call compression, context management, token optimization]
tag: beta
---

Context Compression allows you to manage your agent context while it is running, helping the agent stay within its context window and avoid rate limits or decreases in response quality. 

Think of it like a research assistant who reads lengthy reports and gives you the key bullet points instead of the full documents.



## The Problem: Verbose Tool Results

Without compression, tool results quickly consume your context window:

```
Tool Call 1: 2,500 tokens
Tool Call 2: 5,700 tokens (2,500 + 3,200 new)
Tool Call 3: 8,500 tokens (5,700 + 2,800 new)
Tool Call 4: 12,000 tokens (8,500 + 3,500 new)
...exponential growth
```

This quickly becomes expensive and hits context limits during complex workflows.

## The Solution: Automatic Compression

Context compression summarizes tool results after a threshold:

```
Tool Call 1: 2,500 tokens
Tool Call 2: 5,700 tokens
Tool Call 3: 8,500 tokens
[Compression triggered]
Tool Call 4: 1,300 tokens (800 compressed + 500 new)
...linear growth
```

**Benefits:**
- ✅ Dramatically reduced token costs
- ✅ Stay within context window limits
- ✅ Preserve critical facts and data
- ✅ Automatic compression

## How It Works

Context compression follows a simple pattern:

<Steps>
    <Step title="Enable Compression">
        Set `compress_tool_results=True` on your agent or team. The system monitors tool call results as they come in.
    </Step>
    <Step title="Threshold Reached">
        After the threshold is reached, compression is triggered. All accumulated tool results are summarized.
    </Step>
    <Step title="Intelligent Summarization">
        The compression model preserves key facts (numbers, dates, entities, URLs) while removing boilerplate, redundancy, and filler text.
    </Step>
    <Step title="Customize (Optional)">
        Use [`CompressionManager`](/reference/compression/compression-manager) to control compression - use a cheaper model, adjust the threshold, or provide custom prompts.
    </Step>
</Steps>

## Enable Compression

Turn on `compress_tool_results=True` to automatically compress tool results:

<CodeGroup>
```python Agent
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    compress_tool_results=True,
)

agent.print_response("Research the latest AI announcements")
```

```python Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

web_agent = Agent(
    name="Web Researcher",
    tools=[DuckDuckGoTools()],
)

team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[web_agent],
    compress_tool_results=True,
)

team.print_response("Research the latest AI announcements")
```
</CodeGroup>

## Custom Compression

Provide a [`CompressionManager`](/reference/compression/compression-manager) to customize the compression behavior:

<CodeGroup>
```python Agent
from agno.agent import Agent
from agno.compression.manager import CompressionManager
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

compression_manager = CompressionManager(
    model=OpenAIChat(id="gpt-4o-mini"),  # Use a faster model for compression
    compress_tool_results_limit=2,  # Compress after 2 tool calls (default: 3)
    compress_tool_call_instructions="Your custom compression prompt here...",
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    compression_manager=compression_manager,
)

agent.print_response("Find recent funding rounds for AI startups")
```

```python Team
from agno.agent import Agent
from agno.compression.manager import CompressionManager
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

compression_manager = CompressionManager(
    model=OpenAIChat(id="gpt-4o-mini"),  # Use a faster model for compression
    compress_tool_results_limit=2,  # Compress after 2 tool calls (default: 3)
    compress_tool_call_instructions="Your custom compression prompt here...",
)

web_agent = Agent(
    name="Web Researcher",
    tools=[DuckDuckGoTools()],
)

team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[web_agent],
    compression_manager=compression_manager,
)

team.print_response("Find recent funding rounds for AI startups")
```
</CodeGroup>

<Tip>
Use a faster, cheaper model like `gpt-4o-mini` for compression to reduce latency and cost while keeping your main agent on a more capable model.
</Tip>

## When to Use Context Compression

**✅ Perfect for:**
- Agents with tools that return verbose results (web search, APIs)
- Multi-step workflows with many tool calls
- Long-running sessions where context accumulates
- Production systems where cost matters

**⚠️ Consider alternatives for:**
- Agents with few tool calls (fewer than 3)
- When full tool output details are critical
- Real-time chat where compression latency matters

## Developer Resources

- [CompressionManager Reference](/reference/compression/compression-manager) - Full CompressionManager documentation
- [Agent Reference](/reference/agents/agent) - Agent parameter documentation
- [Team Reference](/reference/teams/team) - Team parameter documentation
- [Cookbook Examples](https://github.com/agno-agi/agno/tree/main/cookbook/agents/context_compression)
