---
title: Cache Sessions
description: Optimize performance by caching sessions in memory
---

Session caching stores the session object in memory to avoid repeated database lookups. This significantly improves performance for high-throughput applications where the same session is accessed frequently.

## How Session Caching Works

When `cache_session=True`:

1. **First Access**: Loads session from database and caches it in memory
2. **Subsequent Access**: Returns cached session without database query
3. **Automatic Updates**: Cache is updated when session is saved
4. **Per-Instance**: Each agent/team instance has its own cache

<Note>
Session caching is **instance-level**, not application-level. If you have multiple agent/team instances accessing the same session, each will have its own cache. For multi-instance deployments, consider using a distributed cache like Redis.
</Note>

## Basic Example

<Tabs>
  <Tab title="Agent">
```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db = PostgresDb(
    host="localhost",
    port=5432,
    user="agno",
    password="agno",
    database="agno",
)

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    session_id="my_session",
    add_history_to_context=True,
    cache_session=True,  # Enable session caching
)

# First run: Loads from database and caches
agent.run("What is machine learning?")

# Second run: Uses cached session (faster - no DB query)
agent.run("Tell me more about neural networks")

# Third run: Still using cache
agent.run("What are transformers?")

# Access the cached session directly
session = agent.get_session()
print(f"Session has {len(session.runs)} runs")
```
  </Tab>

  <Tab title="Team">
```python
from agno.team import Team
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db = PostgresDb(
    host="localhost",
    port=5432,
    user="agno",
    password="agno",
    database="agno",
)

team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[
        Agent(name="Educator", role="You explain ML concepts"),
    ],
    db=db,
    session_id="my_session",
    add_history_to_context=True,
    cache_session=True,  # Enable session caching
)

# First run: Loads from database and caches
team.run("What is machine learning?")

# Second run: Uses cached session (faster - no DB query)
team.run("Tell me more about neural networks")

# Third run: Still using cache
team.run("What are transformers?")

# Access the cached session directly
session = team.get_session()
print(f"Session has {len(session.runs)} runs")
```
  </Tab>
</Tabs>

## Performance Impact

Session caching eliminates database round-trips for session retrieval:

```
Without Caching:
├─ Run 1: DB query (100ms) + Processing (500ms) = 600ms
├─ Run 2: DB query (100ms) + Processing (500ms) = 600ms
└─ Run 3: DB query (100ms) + Processing (500ms) = 600ms
Total: 1800ms

With Caching:
├─ Run 1: DB query (100ms) + Processing (500ms) = 600ms
├─ Run 2: Cache hit (0ms) + Processing (500ms) = 500ms
└─ Run 3: Cache hit (0ms) + Processing (500ms) = 500ms
Total: 1600ms (11% improvement)
```

## High-Volume Application

Perfect for production systems with many sequential runs:

<Tabs>
  <Tab title="Agent">
```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
import time

db = PostgresDb(
    host="localhost",
    port=5432,
    user="agno",
    password="agno",
    database="agno",
)

# Create agent with caching enabled
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    add_history_to_context=True,
    num_history_runs=10,
    cache_session=True,  # Critical for performance
)

# Simulate a conversation with many turns
session_id = "high_volume_session"
messages = [
    "Hello!",
    "What can you help me with?",
    "Tell me about Python",
    "What are decorators?",
    "Show me an example",
    "Explain async/await",
    "What about threading?",
    "Compare threads vs processes",
]

start_time = time.time()

for message in messages:
    agent.run(message, session_id=session_id)

elapsed = time.time() - start_time
print(f"Processed {len(messages)} messages in {elapsed:.2f}s")
print(f"Average: {elapsed/len(messages):.2f}s per message")
```
  </Tab>

  <Tab title="Team">
```python
from agno.team import Team
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
import time

db = PostgresDb(
    host="localhost",
    port=5432,
    user="agno",
    password="agno",
    database="agno",
)

# Create team with caching enabled
team = Team(
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[
        Agent(name="Python Expert", role="You teach Python"),
        Agent(name="Explainer", role="You explain concepts clearly"),
    ],
    db=db,
    add_history_to_context=True,
    num_history_runs=10,
    cache_session=True,  # Critical for performance
)

# Simulate a conversation with many turns
session_id = "high_volume_session"
messages = [
    "Hello!",
    "What can you help me with?",
    "Tell me about Python",
    "What are decorators?",
    "Show me an example",
    "Explain async/await",
    "What about threading?",
    "Compare threads vs processes",
]

start_time = time.time()

for message in messages:
    team.run(message, session_id=session_id)

elapsed = time.time() - start_time
print(f"Processed {len(messages)} messages in {elapsed:.2f}s")
print(f"Average: {elapsed/len(messages):.2f}s per message")
```
  </Tab>
</Tabs>

## Multi-User Support

Cache sessions for multiple users efficiently:

<Tabs>
  <Tab title="Agent">
```python
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db = PostgresDb(
    host="localhost",
    port=5432,
    user="agno",
    password="agno",
    database="agno",
)

# Create a pool of agents with caching
def get_agent_for_user(user_id: str) -> Agent:
    """Get or create an agent instance with caching for a user."""
    
    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        db=db,
        add_history_to_context=True,
        cache_session=True,  # Each agent caches its session
    )
    
    return agent

# Handle multiple users
users = ["user_001", "user_002", "user_003"]

for user_id in users:
    agent = get_agent_for_user(user_id)
    session_id = f"session_{user_id}"
    
    # First message for this user
    agent.run("Hello!", session_id=session_id, user_id=user_id)
    
    # Subsequent messages benefit from caching
    agent.run("How are you?", session_id=session_id, user_id=user_id)
    agent.run("Tell me about AI", session_id=session_id, user_id=user_id)
```
  </Tab>

  <Tab title="Team">
```python
from agno.team import Team
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db = PostgresDb(
    host="localhost",
    port=5432,
    user="agno",
    password="agno",
    database="agno",
)

# Create a pool of teams with caching
def get_team_for_user(user_id: str) -> Team:
    """Get or create a team instance with caching for a user."""
    
    team = Team(
        model=OpenAIChat(id="gpt-4o"),
        members=[
            Agent(name="Assistant", role="You help users"),
        ],
        db=db,
        add_history_to_context=True,
        cache_session=True,  # Each team caches its session
    )
    
    return team

# Handle multiple users
users = ["user_001", "user_002", "user_003"]

for user_id in users:
    team = get_team_for_user(user_id)
    session_id = f"session_{user_id}"
    
    # First message for this user
    team.run("Hello!", session_id=session_id, user_id=user_id)
    
    # Subsequent messages benefit from caching
    team.run("How are you?", session_id=session_id, user_id=user_id)
    team.run("Tell me about AI", session_id=session_id, user_id=user_id)
```
  </Tab>
</Tabs>

## With Async Operations

Caching works seamlessly with async:

<Tabs>
  <Tab title="Agent">
```python
import asyncio
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

async def main():
    db = PostgresDb(
        host="localhost",
        port=5432,
        user="agno",
        password="agno",
        database="agno",
    )
    
    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        db=db,
        session_id="async_session",
        add_history_to_context=True,
        cache_session=True,
    )
    
    # First async run: Loads and caches
    await agent.arun("First message")
    
    # Subsequent runs: Use cache
    await agent.arun("Second message")
    await agent.arun("Third message")
    
    # Access cached session
    session = await agent.aget_session()
    print(f"Cached session has {len(session.runs)} runs")

asyncio.run(main())
```
  </Tab>

  <Tab title="Team">
```python
import asyncio
from agno.team import Team
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

async def main():
    db = PostgresDb(
        host="localhost",
        port=5432,
        user="agno",
        password="agno",
        database="agno",
    )
    
    team = Team(
        model=OpenAIChat(id="gpt-4o"),
        members=[
            Agent(name="Helper", role="You help with tasks"),
        ],
        db=db,
        session_id="async_session",
        add_history_to_context=True,
        cache_session=True,
    )
    
    # First async run: Loads and caches
    await team.arun("First message")
    
    # Subsequent runs: Use cache
    await team.arun("Second message")
    await team.arun("Third message")
    
    # Access cached session
    session = await team.aget_session()
    print(f"Cached session has {len(session.runs)} runs")

asyncio.run(main())
```
  </Tab>
</Tabs>

## Cache Invalidation

The cache is automatically updated when sessions are saved. For manual control:

<Tabs>
  <Tab title="Agent">
```python
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    db=SqliteDb(db_file="tmp/agent.db"),
    cache_session=True,
)

# Run and cache
agent.run("Message 1", session_id="my_session")

# Manually access and modify session
session = agent.get_session(session_id="my_session")
if session:
    # Modify session
    if session.session_data is None:
        session.session_data = {}
    session.session_data["custom_field"] = "custom_value"
    
    # Save updates the cache automatically
    agent.save_session(session=session)

# Next run uses updated cache
agent.run("Message 2", session_id="my_session")
```
  </Tab>

  <Tab title="Team">
```python
from agno.team import Team
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[Agent(name="Helper", role="You help")],
    db=SqliteDb(db_file="tmp/team.db"),
    cache_session=True,
)

# Run and cache
team.run("Message 1", session_id="my_session")

# Manually access and modify session
session = team.get_session(session_id="my_session")
if session:
    # Modify session
    if session.session_data is None:
        session.session_data = {}
    session.session_data["custom_field"] = "custom_value"
    
    # Save updates the cache automatically
    team.save_session(session=session)

# Next run uses updated cache
team.run("Message 2", session_id="my_session")
```
  </Tab>
</Tabs>

## When to Use Caching

| Scenario | Use Caching | Reason |
|----------|-------------|--------|
| Long conversations (10+ turns) | ✅ Yes | Frequent session access |
| Single-turn interactions | ❌ No | No repeated access |
| High concurrency (1000+ req/min) | ✅ Yes | Reduces DB load |
| Multi-instance deployment | ⚠️ Maybe | Consider distributed cache |
| Memory-constrained systems | ❌ No | Cache increases memory usage |
| Production applications | ✅ Yes | Performance critical |

## Best Practices

1. **Enable for Production**: Always enable caching in production for better performance
2. **Monitor Memory**: Each cached session uses memory - monitor for memory leaks
3. **Instance Design**: One agent/team instance per long-lived session works best
4. **Distributed Systems**: For multi-server deployments, consider Redis or similar
5. **Testing**: Test without caching first to ensure correct behavior

## Learn More

- [Session Management Overview](/basics/sessions/session-management/overview)
- [Persisting Sessions](/basics/sessions/persisting-sessions)
- [Sessions Overview](/basics/sessions/overview)

