---
title: Session Summaries
description: Automatically condense long conversations into concise summaries
keywords: [sessions, session summaries, conversation summaries, memory management]
---

As conversations grow longer, passing the entire chat history to your LLM becomes expensive and slow. Session summaries solve this by automatically condensing conversations into concise summaries that capture the key points.

Think of it like taking notes during a long meeting - you don't need a transcript of everything said, just the important bits.

## The Problem: Growing Token Costs

Without summaries, every message adds to your context window:

```
Run 1: 100 tokens
Run 2: 250 tokens (100 history + 150 new)
Run 3: 450 tokens (250 history + 200 new)
Run 4: 750 tokens (450 history + 300 new)
...exponential growth
```

This quickly becomes expensive and hits context limits.

## The Solution: Automatic Summaries

Session summaries condense your history:

```
Run 1: 100 tokens
Run 2: 250 tokens
[Summary created: 50 tokens]
Run 3: 250 tokens (50 summary + 200 new)
Run 4: 350 tokens (50 summary + 300 new)
...linear growth
```

**Benefits:**
- ✅ Dramatically reduced token costs
- ✅ Avoid context window limits
- ✅ Maintain conversation continuity
- ✅ Automatic creation and updates

## How It Works

Session summaries follow a simple three-step pattern:

### 1. Enable Summary Generation
Set `enable_session_summaries=True` on your agent or team. Summaries are automatically created and updated after runs when there are meaningful messages to summarize, then stored in your database.

### 2. Use Summaries in Context
Set `add_session_summary_to_context=True` to include the summary in your messages. Instead of sending dozens of historical messages, only the condensed summary is sent, dramatically reducing tokens while maintaining context.

### 3. Customize (Optional)
Use [`SessionSummaryManager`](/reference/session/summary_manager) to control summary generation - use a cheaper model, customize prompts, or change the summary format. This lets you optimize costs by using a lightweight model for summaries while keeping your main agent powerful.

## Enable Session Summaries

Turn on `enable_session_summaries=True` to have Agno maintain a rolling summary for each session. Summaries sit alongside the stored history and can be reused later to save tokens.

<CodeGroup>
```python Agent
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"),
    session_id="conversation_123",
    enable_session_summaries=True,
)

# Retrieve the summary
summary = agent.get_session_summary(session_id="conversation_123")
if summary:
    print(summary.summary, summary.topics)
```

```python Team
from agno.team import Team
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

team = Team(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"),
    session_id="conversation_123",
    enable_session_summaries=True,
)

# Retrieve the summary
summary = team.get_session_summary(session_id="conversation_123")
if summary:
    print(summary.summary, summary.topics)
```
</CodeGroup>

### Customizing Generation

- Provide a [`SessionSummaryManager`](/reference/session/summary_manager) to specify a cheaper model or custom prompt
- Run summary generation out-of-band by instantiating a lightweight Agent that just calls `get_session_summary` across all sessions
- Combine with `add_session_summary_to_context` to feed summaries back into the model instead of full transcripts

## Use Summary in Context

After a summary exists, flip on `add_session_summary_to_context=True` to send the condensed version (instead of dozens of past turns) with every run.

<CodeGroup>
```python Agent
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="john_session",
    add_session_summary_to_context=True,
)
```

```python Team
from agno.team import Team
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat

db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

team = Team(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="john_session",
    add_session_summary_to_context=True,
)
```
</CodeGroup>

Agno automatically loads the latest summary from storage before each run. You can still mix in recent history:

<CodeGroup>
```python Agent
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    add_session_summary_to_context=True,
    add_history_to_context=True,
    num_history_runs=2,  # Summary for long-term memory, last 2 runs for detail
)
```

```python Team
team = Team(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    add_session_summary_to_context=True,
    add_history_to_context=True,
    num_history_runs=2,  # Summary for long-term memory, last 2 runs for detail
)
```
</CodeGroup>

## Common Patterns

**Two-phase flow:** Run one Agent/Team with `enable_session_summaries=True` to create summaries, then spin up stateless workers that only set `add_session_summary_to_context=True`.

**Async services:** Use `aget_session_summary()` when working with async APIs (`arun`). Summary creation happens automatically during async runs when `enable_session_summaries=True`.

**Fallbacks:** Leave `add_session_summary_to_context=True` even if summaries are temporarily absent; Agno simply skips the injection when there's nothing to load.

## When to Use Session Summaries

**✅ Perfect for:**
- Long-running customer support conversations
- Multi-day or multi-week interactions
- Conversations with 10+ turns
- Production systems where cost matters

**⚠️ Consider alternatives for:**
- Short conversations (fewer than 5 turns)
- When full detail is critical
- Real-time chat with recent context only

