---
title: Speech-to-Text Workflow
mode: wide
---

Build a production-ready speech-to-text workflow using AgentOS with database persistence, multi-format audio support, and structured output. This workflow demonstrates how to orchestrate multiple processing steps for robust audio transcription.

## What You'll Learn

By building this workflow, you'll understand:
- How to structure multi-step workflows with AgentOS
- How to implement session state persistence with PostgreSQL
- How to handle multiple audio formats (MP3, WAV) with automatic conversion
- How to coordinate transcription and post-processing agents
- How to deploy workflows as FastAPI services

## Use Cases

Build transcription services for call centers, create podcast processing pipelines, develop meeting summarization systems, or build voice-to-text APIs for mobile applications.

## How It Works

The workflow follows a four-phase audio processing pipeline:

1. **Echo Input**: Validates and logs the input request with file URL and model configuration
2. **Get Audio Content**: Downloads audio from URL and converts MP3 to WAV if needed for model compatibility
3. **Transcription**: Uses Gemini to transcribe the audio content with speaker identification
4. **Conversion**: Transforms raw transcription into structured output with speaker labels using GPT-4o-mini

The workflow uses PostgreSQL for session persistence, allowing you to resume processing and cache intermediate results.

## Code

```python speech_to_text_workflow.py
"""Speech-to-Text Workflow - Production Audio Transcription Pipeline

This workflow demonstrates how to build a robust audio transcription service using
AgentOS with database persistence and multi-format support.

Key capabilities:
- Multi-format audio support (WAV, MP3)
- Automatic format conversion for model compatibility
- Session state persistence with PostgreSQL
- Structured transcription output
- FastAPI deployment ready
"""

import io
from textwrap import dedent
from typing import Optional

import httpx
import requests
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.media import Audio
from agno.models.google import Gemini
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.utils.log import log_error, log_info
from agno.workflow import Step, Workflow
from agno.workflow.types import StepInput, StepOutput
from pydantic import BaseModel, Field
from pydub import AudioSegment

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(
    db_url=db_url,
    db_schema="ai",
    session_table="invoice_processing_sessions",
)


class Transcription(BaseModel):
    transcript: list[str] = Field(
        ...,
        description="The transcript of the audio conversation. Formatted as a list of strings with speaker labels and logical paragraphs and newlines.",
    )
    description: str = Field(..., description="A description of the audio conversation")
    speakers: list[str] = Field(
        ..., description="The speakers in the audio conversation"
    )


def get_transcription_agent(additional_instructions: Optional[str] = None):
    transcription_agent = Agent(
        model=Gemini(id="gemini-3-flash-preview"),
        markdown=True,
        description="Audio file transcription agent",
        instructions=dedent(f"""Your task is to accurately transcribe the audio into text. You will be given an audio file and you need to transcribe it into text.
            In the transcript, make sure to identify the speakers. If a name is mentioned, use the name in the transcript. If a name is not mentioned, use a placeholder like 'Speaker 1', 'Speaker 2', etc.
            Make sure to include all the content of the audio in the transcript.
            For any audio that is not speech, use the placeholder 'background noise' or 'silence' or 'music' or 'other'.
            Only return the transcript, no other text or formatting.
            {additional_instructions if additional_instructions else ""}"""),
    )
    return transcription_agent


class TranscriptionRequest(BaseModel):
    audio_file: str = (
        "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/sample_audio.wav"
    )
    model_id: str = "gpt-4o-audio-preview"
    additional_instructions: Optional[str] = None


def echo_input_file(step_input: StepInput) -> StepOutput:
    request = step_input.input
    log_info(f"Echoing input file: {request.audio_file}")
    return StepOutput(
        content={
            "file_link": request.audio_file,
            "model_id": request.model_id,
        },
        success=True,
    )


def get_audio_content(step_input: StepInput, session_state) -> StepOutput:
    request = step_input.input
    url = request.audio_file
    if url.endswith(".wav"):
        response = httpx.get(url)
        response.raise_for_status()
        wav_data = response.content
        session_state["audio_content"] = wav_data
        return StepOutput(
            success=True,
        )
    elif url.endswith(".mp3"):
        response = requests.get(url)
        response.raise_for_status()
        mp3_audio = io.BytesIO(response.content)
        audio_segment = AudioSegment.from_file(mp3_audio, format="mp3")
        # Ensure mono and standard sample rate for OpenAI compatibility
        if audio_segment.channels > 1:
            audio_segment = audio_segment.set_channels(1)
        if audio_segment.frame_rate != 16000:
            audio_segment = audio_segment.set_frame_rate(16000)
        wav_io = io.BytesIO()
        audio_segment.export(wav_io, format="wav")
        wav_io.seek(0)
        audio_content = wav_io.read()
        session_state["audio_content"] = audio_content
        return StepOutput(success=True)
    else:
        log_error(f"Unsupported file type: {url}")
        return StepOutput(success=False)


async def transcription_agent_executor(
    step_input: StepInput, session_state
) -> StepOutput:
    audio_content = session_state["audio_content"]
    transcription_agent = get_transcription_agent(
        additional_instructions=step_input.input.additional_instructions
    )
    response = await transcription_agent.arun(
        input="Give a transcript of the audio conversation",
        audio=[Audio(content=audio_content, format="wav")],
    )
    print(response.content)
    session_state["transcription"] = response.content
    return StepOutput(
        success=True,
    )


async def convert_transcription_to_output(
    step_input: StepInput, session_state
) -> StepOutput:
    transcription = session_state["transcription"]
    agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        instructions="""You are a helpful assistant that converts a transcription of an audio conversation into a structured output.""",
        output_schema=Transcription,
    )

    response = await agent.arun(input=transcription)

    return StepOutput(content=response.content, success=True)


# Define workflow steps
echo_input_step = Step(name="Echo Input", executor=echo_input_file)
get_audio_content_step = Step(name="Get Audio Content", executor=get_audio_content)
transcription_step = Step(name="Transcription", executor=transcription_agent_executor)
conversion_step = Step(name="Conversion", executor=convert_transcription_to_output)

# Workflow definition
speech_to_text_workflow = Workflow(
    name="Speech to text workflow",
    description="""
        Transcribe audio file using transcription agent
        """,
    input_schema=TranscriptionRequest,
    steps=[
        echo_input_step,
        get_audio_content_step,
        transcription_step,
        conversion_step,
    ],
    db=db,
)


agent_os = AgentOS(
    workflows=[speech_to_text_workflow],
)

app = agent_os.get_app()
if __name__ == "__main__":
    # Serves a FastAPI app exposed by AgentOS. Use reload=True for local dev.
    agent_os.serve(app="stt_workflow:app", reload=True)
```

## What to Expect

The workflow starts a FastAPI server that exposes endpoints for submitting transcription requests. When you submit an audio file URL:

1. The workflow validates and logs the input
2. Downloads and converts the audio to WAV format if needed
3. Transcribes the audio using Gemini with speaker identification
4. Converts the raw transcription to structured output

The final output is a `Transcription` object with:
- **transcript**: List of transcript segments with speaker labels
- **description**: Summary of the audio content
- **speakers**: List of identified speakers

Session state is persisted to PostgreSQL, allowing workflow resumption and caching.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Start PostgreSQL">
    Run PostgreSQL using Docker:
    ```bash
    docker run -d \
      --name pgvector \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -e POSTGRES_DB=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    ```
  </Step>

  <Step title="Set your API keys">
    ```bash
    export GOOGLE_API_KEY=xxx
    export OPENAI_API_KEY=xxx
    ```
  </Step>

  <Step title="Install libraries">
    ```bash
    pip install -U agno google-genai openai httpx pydub psycopg[binary] fastapi uvicorn
    ```
  </Step>

  <Step title="Run Workflow">
    <CodeGroup>
    ```bash Mac
    python speech_to_text_workflow.py
    ```

    ```bash Windows
    python speech_to_text_workflow.py
    ```
    </CodeGroup>
  </Step>
</Steps>

## Next Steps

- Add additional workflow steps for sentiment analysis or topic extraction
- Implement caching to avoid re-processing the same audio files
- Add support for additional audio formats (M4A, FLAC, OGG)
- Explore [AgentOS](/agent-os/overview) for advanced deployment options
