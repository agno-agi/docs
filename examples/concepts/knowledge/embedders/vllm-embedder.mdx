---
title: vLLM Embedder
---

## Code

```python cookbook/knowledge/embedders/vllm_embedder.py
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase, PDFReader
from agno.knowledge.embedder.vllm import VLLMEmbedder
from agno.models.openai import OpenAIChat
from agno.vectordb.pgvector import PgVector

# Create knowledge base with vLLM embedder (local mode)
knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    vector_db=PgVector(
        table_name="vllm_documents",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        embedder=VLLMEmbedder(
            id="intfloat/e5-mistral-7b-instruct",
            dimensions=4096,
        ),
    ),
    reader=PDFReader(chunk=True),
)
knowledge_base.load(recreate=False)

# Create agent with knowledge
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    knowledge=knowledge_base,
    search_knowledge=True,
    show_tool_calls=True,
    markdown=True,
)

agent.print_response("What is the main topic?", markdown=True)
```

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
    ```bash
    pip install -U agno vllm openai sqlalchemy psycopg[binary] pgvector pypdf
    ```
  </Step>

  <Step title="Set environment variables">
    ```bash
    export OPENAI_API_KEY=xxx
    ```
  </Step>

  <Step title="Run PgVector">
    ```bash
    docker run -d \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -e PGDATA=/var/lib/postgresql/data/pgdata \
      -v pgvolume:/var/lib/postgresql/data \
      -p 5532:5432 \
      --name pgvector \
      agno/pgvector:16
    ```
  </Step>

  <Step title="Run the agent">
    ```bash
    python cookbook/knowledge/embedders/vllm_embedder.py
    ```
  </Step>
</Steps>

## Notes

- This example uses **local mode** where vLLM loads the model directly (no server needed)
- For **remote mode**, use `base_url` parameter: `VLLMEmbedder(base_url="http://localhost:8000/v1")`
- GPU with ~14GB VRAM required for e5-mistral-7b-instruct model
- For CPU-only or lower memory, use smaller models like `BAAI/bge-small-en-v1.5`
