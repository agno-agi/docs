---
title: Open Responses
sidebarTitle: Open Responses
---

Base class for interacting with providers that implement the [Open Responses API specification](https://openresponses.org). This provides a foundation for multi-provider, interoperable LLM interfaces based on the OpenAI Responses API.

Providers that implement this spec include Ollama (v0.13.3+) and OpenRouter.

## Key Differences from OpenAI Responses

- Configurable `base_url` for pointing to different API endpoints
- Stateless by default (no `previous_response_id` chaining)
- Flexible `api_key` handling for providers that don't require authentication

## Parameters

| Parameter    | Type               | Default            | Description                                        |
| ------------ | ------------------ | ------------------ | -------------------------------------------------- |
| `id`         | `str`              | `"not-provided"`   | The ID of the model to use                         |
| `name`       | `str`              | `"OpenResponses"`  | The name of the model                              |
| `provider`   | `str`              | `"OpenResponses"`  | The provider of the model                          |
| `api_key`    | `Optional[str]`    | `"not-provided"`   | The API key for authentication                     |
| `store`      | `Optional[bool]`   | `False`            | Whether to store responses (disabled by default for compatible providers) |

## Usage

For most use cases, prefer the provider-specific classes:
- [OllamaResponses](/reference/models/ollama-responses) for Ollama
- [OpenRouterResponses](/reference/models/openrouter-responses) for OpenRouter

```python
from agno.agent import Agent
from agno.models.openai import OpenResponses

agent = Agent(
    model=OpenResponses(
        id="your-model-id",
        base_url="https://your-provider.com/v1",
        api_key="your-api-key",
    ),
)

agent.print_response("Hello!")
```
